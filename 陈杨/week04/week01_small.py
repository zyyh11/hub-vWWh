import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
# BertForSequenceClassification bert ç”¨äº æ–‡æœ¬åˆ†ç±»
# Trainerï¼š ç›´æ¥å®ç° æ­£å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€å‚æ•°æ›´æ–°
# TrainingArgumentsï¼š è¶…å‚æ•°ã€å®éªŒè®¾ç½®

from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import numpy as np
import json

def load_product_data():
    """åŠ è½½å•†å“åˆ†ç±»æ•°æ®"""
    data_dir = "winwin_inc/product-classification-hiring-demo"
    
    # è¯»å–è®­ç»ƒæ•°æ®
    train_data = []
    with open(f"{data_dir}/train.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                train_data.append({
                    'text': item['product_name'],
                    'label': item['category']
                })
    
    # è½¬æ¢ä¸ºDataFrame
    dataset_df = pd.DataFrame(train_data)
    return dataset_df

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
print("åŠ è½½å•†å“åˆ†ç±»æ•°æ®...")
dataset_df = load_product_data()

print(f"æ•°æ®é›†å¤§å°: {len(dataset_df)}")
print(f"åˆ—å: {dataset_df.columns.tolist()}")
print(f"å‰å‡ è¡Œæ•°æ®:")
print(dataset_df.head())

# åˆå§‹åŒ– LabelEncoderï¼Œç”¨äºå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
lbl = LabelEncoder()
# æ‹Ÿåˆæ•°æ®å¹¶è½¬æ¢æ ‡ç­¾
labels = lbl.fit_transform(dataset_df['label'].values[:500])
# æå–å‰500ä¸ªæ–‡æœ¬å†…å®¹
texts = list(dataset_df['text'].values[:500])

print(f"\næ ‡ç­¾ç±»åˆ«: {lbl.classes_}")
print(f"æ ‡ç­¾æ•°é‡: {len(labels)}")
print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")

# åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
x_train, x_test, train_labels, test_labels = train_test_split(
    texts,             # æ–‡æœ¬æ•°æ®
    labels,            # å¯¹åº”çš„æ•°å­—æ ‡ç­¾
    test_size=0.2,     # æµ‹è¯•é›†æ¯”ä¾‹ä¸º20%
    stratify=labels    # ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒä¸€è‡´
)




# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('models/bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('models/bert-base-chinese', num_labels=10)

device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
model.to(device)
# ä½¿ç”¨åˆ†è¯å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–‡æœ¬è¿›è¡Œç¼–ç 
# truncation=Trueï¼šå¦‚æœæ–‡æœ¬è¿‡é•¿åˆ™æˆªæ–­
# padding=Trueï¼šå¯¹é½æ‰€æœ‰åºåˆ—é•¿åº¦ï¼Œå¡«å……åˆ°æœ€é•¿
# max_length=64ï¼šæœ€å¤§åºåˆ—é•¿åº¦
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=64)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=64)

# å°†ç¼–ç åçš„æ•°æ®å’Œæ ‡ç­¾è½¬æ¢ä¸º Hugging Face `datasets` åº“çš„ Dataset å¯¹è±¡
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],           # æ–‡æœ¬çš„token ID
    'attention_mask': train_encodings['attention_mask'], # æ³¨æ„åŠ›æ©ç 
    'labels': train_labels                               # å¯¹åº”çš„æ ‡ç­¾
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels
})





# å®šä¹‰ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def compute_metrics(eval_pred):
    # eval_pred æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡å‹é¢„æµ‹çš„ logits å’ŒçœŸå®çš„æ ‡ç­¾
    logits, labels = eval_pred
    # æ‰¾åˆ° logits ä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„ç±»åˆ«
    predictions = np.argmax(logits, axis=-1)
    # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡å¹¶è¿”å›ä¸€ä¸ªå­—å…¸
    return {'accuracy': (predictions == labels).mean()}

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',              # è®­ç»ƒè¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜æ¨¡å‹å’ŒçŠ¶æ€
    num_train_epochs=4,                  # è®­ç»ƒçš„æ€»è½®æ•°
    per_device_train_batch_size=32,      # è®­ç»ƒæ—¶æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰çš„æ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=32,       # è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
    warmup_steps=500,                    # å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒï¼Œ step å®šä¹‰ä¸º ä¸€æ¬¡ æ­£å‘ä¼ æ’­ + å‚æ•°æ›´æ–°
    weight_decay=0.01,                   # æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    logging_dir='./logs',                # æ—¥å¿—å­˜å‚¨ç›®å½•
    logging_steps=100,                   # æ¯éš”100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    eval_strategy="epoch",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    save_strategy="epoch",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    load_best_model_at_end=True,         # è®­ç»ƒç»“æŸååŠ è½½æ•ˆæœæœ€å¥½çš„æ¨¡å‹
)

# å®ä¾‹åŒ– Trainer ç®€åŒ–æ¨¡å‹è®­ç»ƒä»£ç 
trainer = Trainer(
    model=model,                         # è¦è®­ç»ƒçš„æ¨¡å‹
    args=training_args,                  # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,         # è®­ç»ƒæ•°æ®é›†
    eval_dataset=test_dataset,           # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,     # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
)

# æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ï¼Œæ•°æ®è·å–ï¼Œepoch batch å¾ªç¯ï¼Œæ¢¯åº¦è®¡ç®— + å‚æ•°æ›´æ–°

# å¼€å§‹è®­ç»ƒæ¨¡å‹
trainer.train()
# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
trainer.evaluate()

# trainer æ˜¯æ¯”è¾ƒç®€å•ï¼Œé€‚åˆè®­ç»ƒè¿‡ç¨‹æ¯”è¾ƒè§„èŒƒåŒ–çš„æ¨¡å‹
# å¦‚æœæˆ‘è¦å®šåˆ¶åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œtraineræ— æ³•æ»¡è¶³

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
print("ğŸ’¾ ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
model.save_pretrained("./final_trained_model")
tokenizer.save_pretrained("./final_trained_model")

# æµ‹è¯•æ–°æ ·æœ¬åˆ†ç±»æ•ˆæœ
def predict_text_category(text):
    """é¢„æµ‹æ–‡æœ¬ç±»åˆ«"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=64)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class_id = outputs.logits.argmax().item()
        confidence = torch.softmax(outputs.logits, dim=-1)[0][predicted_class_id].item()
    predicted_label = lbl.inverse_transform([predicted_class_id])[0]
    return predicted_label, confidence

# æµ‹è¯•æ–°çš„æ ·æœ¬
print("\nğŸ§ª æ–°æ ·æœ¬åˆ†ç±»æµ‹è¯•:")
test_samples = [
    "è’™ç‰›çº¯ç‰›å¥¶250ml",
    "å¯å£å¯ä¹330ml",
    "è¥¿å‡¤é…’55åº¦"
]

for sample in test_samples:
    predicted_label, confidence = predict_text_category(sample)
    print(f"æ–‡æœ¬: {sample}")
    print(f"é¢„æµ‹ç±»åˆ«: {predicted_label}")
    print(f"ç½®ä¿¡åº¦: {confidence:.4f}")
    print("-" * 50)

print("\nâœ… æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
