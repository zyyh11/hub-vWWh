import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import numpy as np
import json
import joblib
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import RandomOverSampler

def load_product_data():
    data_dir = "winwin_inc/product-classification-hiring-demo"
    # è¯»å–è®­ç»ƒæ•°æ®
    train_data = []
    with open(f"{data_dir}/train.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                train_data.append({
                    'text': item['product_name'],
                    'label': item['category']
                })

    # è¯»å–æµ‹è¯•æ•°æ®
    test_data = []
    with open(f"{data_dir}/test.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                test_data.append({
                    'text': item['product_name'],
                    'label': item['category']
                })

    # åˆå¹¶æ•°æ®ç”¨äºæ ‡ç­¾ç¼–ç 
    all_data = train_data + test_data
    dataset_df = pd.DataFrame(all_data)

    print(f"æ€»æ•°æ®é‡: {len(dataset_df)} æ¡è®°å½•")
    print(f"å•†å“ç±»åˆ«: {dataset_df['label'].nunique()} ä¸ª")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{dataset_df['label'].value_counts()}")

    return train_data, test_data

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
train_data, test_data = load_product_data()

# è½¬æ¢ä¸ºDataFrameæ ¼å¼
train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)

print(f"\nåŸå§‹æ•°æ®é›†åˆ’åˆ†:")
print(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
print(f"æµ‹è¯•é›†: {len(test_df)} æ¡")

# å¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ - ä½¿ç”¨è¿‡é‡‡æ ·
print("\nå¤„ç†æ•°æ®ä¸å¹³è¡¡é—®é¢˜...")
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(
    train_df[['text']], 
    train_df['label']
)
train_df_balanced = pd.DataFrame({
    'text': X_resampled['text'].values,
    'label': y_resampled
})

print(f"å¹³è¡¡åè®­ç»ƒé›†: {len(train_df_balanced)} æ¡")

# åˆå§‹åŒ– LabelEncoderï¼Œç”¨äºå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
lbl = LabelEncoder()
# æ‹Ÿåˆæ‰€æœ‰æ ‡ç­¾æ•°æ®
all_labels = pd.concat([train_df_balanced['label'], test_df['label']])
lbl.fit(all_labels.values)

# è½¬æ¢è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ‡ç­¾
train_labels = lbl.transform(train_df_balanced['label'].values)
test_labels = lbl.transform(test_df['label'].values)

# æå–æ–‡æœ¬å†…å®¹
x_train = list(train_df_balanced['text'].values)
x_test = list(test_df['text'].values)

print(f"\næœ€ç»ˆæ•°æ®é›†åˆ’åˆ†:")
print(f"è®­ç»ƒé›†: {len(x_train)} æ¡")
print(f"æµ‹è¯•é›†: {len(x_test)} æ¡")
print(f"ç±»åˆ«æ•°: {len(lbl.classes_)}")
print(f"ç±»åˆ«åˆ—è¡¨: {list(lbl.classes_)}")

# ä½¿ç”¨æ›´å¥½çš„é¢„è®­ç»ƒæ¨¡å‹
print("\nåŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
tokenizer = BertTokenizer.from_pretrained('models/bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('models/bert-base-chinese', num_labels=len(lbl.classes_))

# ä½¿ç”¨æ›´é•¿çš„åºåˆ—é•¿åº¦ä»¥é€‚åº”å•†å“åç§°
print("å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ...")
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=128)

# å°†ç¼–ç åçš„æ•°æ®å’Œæ ‡ç­¾è½¬æ¢ä¸º Hugging Face `datasets` åº“çš„ Dataset å¯¹è±¡
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],  # æ–‡æœ¬çš„token ID
    'attention_mask': train_encodings['attention_mask'],  # æ³¨æ„åŠ›æ©ç 
    'labels': train_labels  # å¯¹åº”çš„æ ‡ç­¾
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels
})

# å®šä¹‰ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def compute_metrics(eval_pred):
    # eval_pred æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡å‹é¢„æµ‹çš„ logits å’ŒçœŸå®çš„æ ‡ç­¾
    logits, labels = eval_pred
    # æ‰¾åˆ° logits ä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„ç±»åˆ«
    predictions = np.argmax(logits, axis=-1)
    # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡å¹¶è¿”å›ä¸€ä¸ªå­—å…¸
    accuracy = (predictions == labels).mean()

    # è®¡ç®—F1åˆ†æ•°
    from sklearn.metrics import f1_score, precision_score, recall_score
    f1 = f1_score(labels, predictions, average='weighted')
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# é…ç½®ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°
print("é…ç½®è®­ç»ƒå‚æ•°...")
training_args = TrainingArguments(
    output_dir='./product_results_optimized',  # è®­ç»ƒè¾“å‡ºç›®å½•
    num_train_epochs=2,  # å¢åŠ è®­ç»ƒè½®æ•°
    per_device_train_batch_size=64,  # æ‰¹é‡å¤§å°
    per_device_eval_batch_size=64,
    gradient_accumulation_steps=1,   # å‡å°‘æ¢¯åº¦ç´¯ç§¯
    learning_rate=2e-5,  # è®¾ç½®åˆé€‚çš„å­¦ä¹ ç‡
    warmup_ratio=0.1,  # ä½¿ç”¨æ¯”ä¾‹è€Œä¸æ˜¯å›ºå®šæ­¥æ•°
    weight_decay=0.01,
    logging_dir='./logs_optimized',
    logging_steps=100,  # æ›´é¢‘ç¹çš„æ—¥å¿—è®°å½•
    evaluation_strategy="steps",  # æ¯éš”ä¸€å®šæ­¥æ•°è¯„ä¼°
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    load_best_model_at_end=True,
    metric_for_best_model="f1",  # ä½¿ç”¨F1ä½œä¸ºæœ€ä¼˜æ¨¡å‹æ ‡å‡†
    greater_is_better=True,
    seed=42,  # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
)

# å®ä¾‹åŒ– Trainer ç®€åŒ–æ¨¡å‹è®­ç»ƒä»£ç 
print("åˆå§‹åŒ–è®­ç»ƒå™¨...")
trainer = Trainer(
    model=model,  # è¦è®­ç»ƒçš„æ¨¡å‹
    args=training_args,  # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,  # è®­ç»ƒæ•°æ®é›†
    eval_dataset=test_dataset,  # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,  # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
)

# æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹
print("ğŸš€ å¼€å§‹è®­ç»ƒå•†å“åˆ†ç±»æ¨¡å‹...")
# å¼€å§‹è®­ç»ƒæ¨¡å‹
trainer.train()

print("ğŸ¯ åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
eval_results = trainer.evaluate()
print(f"\nğŸ“ˆ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
print("ğŸ’¾ ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
model.save_pretrained("./final_product_model_optimized")
tokenizer.save_pretrained("./final_product_model_optimized")
joblib.dump(lbl, "./final_product_model_optimized/label_encoder.pkl")
